{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T15:29:07.341150Z",
     "iopub.status.busy": "2025-04-30T15:29:07.340969Z",
     "iopub.status.idle": "2025-04-30T15:29:16.742868Z",
     "shell.execute_reply": "2025-04-30T15:29:16.741824Z",
     "shell.execute_reply.started": "2025-04-30T15:29:07.341130Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install snntorch\n",
    "!pip install tonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T15:29:18.974219Z",
     "iopub.status.busy": "2025-04-30T15:29:18.973930Z",
     "iopub.status.idle": "2025-04-30T15:29:23.339668Z",
     "shell.execute_reply": "2025-04-30T15:29:23.338997Z",
     "shell.execute_reply.started": "2025-04-30T15:29:18.974195Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional as SF\n",
    "\n",
    "import tonic\n",
    "import tonic.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T15:29:29.421555Z",
     "iopub.status.busy": "2025-04-30T15:29:29.421091Z",
     "iopub.status.idle": "2025-04-30T15:29:29.475741Z",
     "shell.execute_reply": "2025-04-30T15:29:29.474697Z",
     "shell.execute_reply.started": "2025-04-30T15:29:29.421527Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path='/data/dvs' # Directory where DVS dataset is stored\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") # Use GPU if available\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T15:29:32.278866Z",
     "iopub.status.busy": "2025-04-30T15:29:32.278432Z",
     "iopub.status.idle": "2025-04-30T15:33:31.230496Z",
     "shell.execute_reply": "2025-04-30T15:33:31.229569Z",
     "shell.execute_reply.started": "2025-04-30T15:29:32.278827Z"
    }
   },
   "outputs": [],
   "source": [
    "train = tonic.datasets.DVSGesture(data_path, train=True)\n",
    "test = tonic.datasets.DVSGesture(data_path, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T15:33:31.232081Z",
     "iopub.status.busy": "2025-04-30T15:33:31.231762Z",
     "iopub.status.idle": "2025-04-30T15:33:31.282439Z",
     "shell.execute_reply": "2025-04-30T15:33:31.281554Z",
     "shell.execute_reply.started": "2025-04-30T15:33:31.232051Z"
    }
   },
   "outputs": [],
   "source": [
    "transforms1 = tonic.transforms.Compose([\n",
    "    tonic.transforms.Denoise(filter_time=10000), # removes outlier events with inactive surrounding pixels for 10ms\n",
    "    tonic.transforms.Downsample(sensor_size=tonic.datasets.DVSGesture.sensor_size, target_size=(32,32)), # downsampling image\n",
    "    tonic.transforms.ToFrame(sensor_size=(32,32,2), n_time_bins=150), # n_frames frames per trail\n",
    "])\n",
    "\n",
    "transforms2 = tonic.transforms.Compose([\n",
    "    tonic.transforms.Denoise(filter_time=10000), # removes outlier events with inactive surrounding pixels for 10ms\n",
    "    tonic.transforms.Downsample(sensor_size=tonic.datasets.DVSGesture.sensor_size, target_size=(32,32)), # downsampling image\n",
    "    tonic.transforms.ToFrame(sensor_size=(32,32,2), n_time_bins=150), # n_frames frames per trail\n",
    "])\n",
    "\n",
    "\n",
    "train2 = tonic.datasets.DVSGesture(data_path, transform=transforms1, train=True)\n",
    "test2 = tonic.datasets.DVSGesture(data_path, transform=transforms2, train=False)\n",
    "\n",
    "cached_train = tonic.DiskCachedDataset(train2, cache_path='/temp/dvsgesture/train')\n",
    "cached_test = tonic.DiskCachedDataset(test2, cache_path='/temp/dvsgesture/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T15:34:56.590455Z",
     "iopub.status.busy": "2025-04-30T15:34:56.590153Z",
     "iopub.status.idle": "2025-04-30T15:34:56.594734Z",
     "shell.execute_reply": "2025-04-30T15:34:56.593939Z",
     "shell.execute_reply.started": "2025-04-30T15:34:56.590430Z"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"num_epochs_eval\": 150,  # Number of epochs to train for (per trial)\n",
    "    \"batch_size\": 32,  # Batch size\n",
    "    \"seed\": 0,  # Random seed\n",
    "    # Network parameters\n",
    "    \"batch_norm\": True,  # Whether or not to use batch normalization\n",
    "    \"dropout\": 0.203,  # Dropout rate\n",
    "    \"beta\": 0.72,  # Decay rate parameter (beta)\n",
    "    \"threshold\": 2.5,  # Threshold parameter (theta)\n",
    "    \"lr\": 2.4e-3,  # Initial learning rate\n",
    "    \"slope\": 9.7,  # Slope value (k)\n",
    "    # Fixed params\n",
    "    \"num_steps\": 150,  # Number of timesteps to encode input for 100\n",
    "    \"correct_rate\": 0.8,  # Correct rate\n",
    "    \"incorrect_rate\": 0.2,  # Incorrect rate\n",
    "    \"betas\": (0.9, 0.999),  # Adam optimizer beta values\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T15:34:58.284659Z",
     "iopub.status.busy": "2025-04-30T15:34:58.284348Z",
     "iopub.status.idle": "2025-04-30T15:35:43.200710Z",
     "shell.execute_reply": "2025-04-30T15:35:43.199790Z",
     "shell.execute_reply.started": "2025-04-30T15:34:58.284633Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = config[\"batch_size\"] # Batches of 32 samples\n",
    "trainloader = DataLoader(cached_train, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=tonic.collation.PadTensors(batch_first=False))\n",
    "frames, target = next(iter(trainloader))\n",
    "testloader = DataLoader(cached_test, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=tonic.collation.PadTensors(batch_first=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T15:35:43.202465Z",
     "iopub.status.busy": "2025-04-30T15:35:43.202118Z",
     "iopub.status.idle": "2025-04-30T15:35:43.445033Z",
     "shell.execute_reply": "2025-04-30T15:35:43.444036Z",
     "shell.execute_reply.started": "2025-04-30T15:35:43.202433Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.thr = config[\"threshold\"]\n",
    "        self.slope = config[\"slope\"]\n",
    "        self.beta = config[\"beta\"]\n",
    "        self.num_steps = config[\"num_steps\"]\n",
    "        self.batch_norm = config[\"batch_norm\"]\n",
    "        self.p1 = config[\"dropout\"]\n",
    "        self.spike_grad = surrogate.fast_sigmoid(self.slope)\n",
    "        self.init_net()\n",
    "\n",
    "    def init_net(self):\n",
    "        self.conv1 = nn.Conv2d(2, 16, 5, bias=False)\n",
    "        self.conv1_bn = nn.BatchNorm2d(16)\n",
    "        self.lif1 = snn.Leaky(self.beta, threshold=self.thr, spike_grad=self.spike_grad)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5, bias=False)\n",
    "        self.conv2_bn = nn.BatchNorm2d(32)\n",
    "        self.lif2 = snn.Leaky(self.beta, threshold=self.thr, spike_grad=self.spike_grad)\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 11)\n",
    "        self.lif3 = snn.Leaky(self.beta, threshold=self.thr, spike_grad=self.spike_grad)\n",
    "        self.dropout = nn.Dropout(self.p1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden states and outputs at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "        # Record the final layer\n",
    "        spk3_rec = []\n",
    "        mem3_rec = []\n",
    "        for step in range(x.size(0)):\n",
    "            cur1 = F.avg_pool2d(self.conv1(x[step]), 2)\n",
    "            if self.batch_norm:\n",
    "                cur1 = self.conv1_bn(cur1)\n",
    "\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = F.avg_pool2d(self.conv2(spk1), 2)\n",
    "            if self.batch_norm:\n",
    "                cur2 = self.conv2_bn(cur2)\n",
    "\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            cur3 = self.fc1(spk2.flatten(1))\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            spk3_rec.append(spk3)\n",
    "            mem3_rec.append(mem3)\n",
    "\n",
    "        return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)\n",
    "\n",
    "net = Net(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T15:35:43.446907Z",
     "iopub.status.busy": "2025-04-30T15:35:43.446593Z",
     "iopub.status.idle": "2025-04-30T15:35:43.456876Z",
     "shell.execute_reply": "2025-04-30T15:35:43.456016Z",
     "shell.execute_reply.started": "2025-04-30T15:35:43.446875Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(config, net, trainloader, criterion, optimizer, device=device, scheduler=None):\n",
    "    net.train()\n",
    "    loss_accum = []\n",
    "    i = 0\n",
    "    for data, labels in trainloader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        spk_rec, _ = net(data.permute(0, 1, 2, 3, 4))\n",
    "\n",
    "        loss = criterion(spk_rec, labels.long())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        loss_accum.append(loss.item() / config[\"num_steps\"])\n",
    "    acc = SF.accuracy_rate(spk_rec, labels.long())\n",
    "    return loss_accum, acc\n",
    "\n",
    "def test(config, net, testloader, device=device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs, _ = net(images.permute(0, 1, 2, 3, 4))\n",
    "            accuracy = SF.accuracy_rate(outputs, labels.long())\n",
    "            total += labels.size(0)\n",
    "            correct += accuracy * labels.size(0)\n",
    "\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T15:35:43.458079Z",
     "iopub.status.busy": "2025-04-30T15:35:43.457806Z",
     "iopub.status.idle": "2025-04-30T15:35:45.425000Z",
     "shell.execute_reply": "2025-04-30T15:35:45.424086Z",
     "shell.execute_reply.started": "2025-04-30T15:35:43.458054Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=config[\"lr\"], betas=config[\"betas\"])\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "criterion = SF.mse_count_loss(correct_rate=config[\"correct_rate\"],\n",
    "    incorrect_rate=config[\"incorrect_rate\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T15:35:45.426528Z",
     "iopub.status.busy": "2025-04-30T15:35:45.426105Z",
     "iopub.status.idle": "2025-04-30T16:33:58.691217Z",
     "shell.execute_reply": "2025-04-30T16:33:58.690297Z",
     "shell.execute_reply.started": "2025-04-30T15:35:45.426502Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "\n",
    "print(f\"=======Training Network=======\")\n",
    "# Train\n",
    "for epoch in range(config['num_epochs_eval']):\n",
    "    loss, acc= train(config, net, trainloader, criterion, optimizer,\n",
    "        device)\n",
    "    loss_list = loss_list + loss\n",
    "    print(f'Train accuracy: {acc*100}')\n",
    "    # Test\n",
    "    test_accuracy = test(config, net, testloader, device)\n",
    "    print(f\"Epoch: {epoch} \\tTest Accuracy: {test_accuracy}\")\n",
    "torch.save(net.state_dict(), 'DVSGesturefp32.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
