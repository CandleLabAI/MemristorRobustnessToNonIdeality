{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T15:45:34.816301Z",
     "iopub.status.busy": "2025-05-02T15:45:34.816107Z",
     "iopub.status.idle": "2025-05-02T15:45:39.782486Z",
     "shell.execute_reply": "2025-05-02T15:45:39.781514Z",
     "shell.execute_reply.started": "2025-05-02T15:45:34.816283Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install snntorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T15:45:47.108652Z",
     "iopub.status.busy": "2025-05-02T15:45:47.107908Z",
     "iopub.status.idle": "2025-05-02T15:45:51.288858Z",
     "shell.execute_reply": "2025-05-02T15:45:51.288125Z",
     "shell.execute_reply.started": "2025-05-02T15:45:47.108612Z"
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import torch, torch.nn as nn\n",
    "import snntorch as snn\n",
    "import snntorch.functional as SF\n",
    "\n",
    "from torch import randn_like\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T15:45:51.290316Z",
     "iopub.status.busy": "2025-05-02T15:45:51.290007Z",
     "iopub.status.idle": "2025-05-02T15:45:59.697889Z",
     "shell.execute_reply": "2025-05-02T15:45:59.697059Z",
     "shell.execute_reply.started": "2025-05-02T15:45:51.290297Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "data_path='/data/fmnist' # Directory where MNIST dataset is stored\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") # Use GPU if available\n",
    "\n",
    "# Define a transform to normalize data\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((28, 28)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "# Download and load the training and test FashionMNIST datasets\n",
    "fmnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transform)\n",
    "fmnist_test = datasets.FashionMNIST(data_path, train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T15:45:59.699020Z",
     "iopub.status.busy": "2025-05-02T15:45:59.698680Z",
     "iopub.status.idle": "2025-05-02T15:45:59.703980Z",
     "shell.execute_reply": "2025-05-02T15:45:59.703149Z",
     "shell.execute_reply.started": "2025-05-02T15:45:59.698994Z"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"num_epochs\": 100,  # Number of epochs to train for (per trial)\n",
    "    \"batch_size\": 128,  # Batch size\n",
    "    \"seed\": 0,  # Random seed\n",
    "\n",
    "\n",
    "    # Network parameters\n",
    "    \"batch_norm\": True,  # Whether or not to use batch normalization\n",
    "    \"dropout\": 0.13,  # Dropout rate\n",
    "    \"beta\": 0.39,  # Decay rate parameter (beta)\n",
    "    \"threshold\": 1.5,  # Threshold parameter (theta)\n",
    "    \"lr\": 2.0e-3,  # Initial learning rate\n",
    "    \"slope\": 7.7,  # Slope value (k)\n",
    "\n",
    "    # Fixed params\n",
    "    \"num_steps\": 100,  # Number of timesteps to encode input for\n",
    "    \"correct_rate\": 0.8,  # Correct rate\n",
    "    \"incorrect_rate\": 0.2,  # Incorrect rate\n",
    "    \"betas\": (0.9, 0.999),  # Adam optimizer beta values\n",
    "    \"eta_min\": 0,  # Minimum learning rate\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T15:45:59.706296Z",
     "iopub.status.busy": "2025-05-02T15:45:59.706062Z",
     "iopub.status.idle": "2025-05-02T15:45:59.724313Z",
     "shell.execute_reply": "2025-05-02T15:45:59.723560Z",
     "shell.execute_reply.started": "2025-05-02T15:45:59.706276Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = config['batch_size']\n",
    "trainloader = DataLoader(fmnist_train, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(fmnist_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T15:45:59.725756Z",
     "iopub.status.busy": "2025-05-02T15:45:59.725173Z",
     "iopub.status.idle": "2025-05-02T15:45:59.779468Z",
     "shell.execute_reply": "2025-05-02T15:45:59.778778Z",
     "shell.execute_reply.started": "2025-05-02T15:45:59.725720Z"
    }
   },
   "outputs": [],
   "source": [
    "from snntorch import surrogate\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.thr = config[\"threshold\"]\n",
    "        self.slope = config[\"slope\"]\n",
    "        self.beta = config[\"beta\"]\n",
    "        self.num_steps = config[\"num_steps\"]\n",
    "        self.batch_norm = config[\"batch_norm\"]\n",
    "        self.p1 = config[\"dropout\"]\n",
    "        self.spike_grad = surrogate.fast_sigmoid(self.slope)\n",
    "        # self.noiser = Noisy_Inference.apply\n",
    "\n",
    "        # Initialize Layers\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5, bias=False)\n",
    "        self.conv1_bn = nn.BatchNorm2d(16)\n",
    "        self.lif1 = snn.Leaky(self.beta, threshold=self.thr, spike_grad=self.spike_grad)\n",
    "        self.conv2 = nn.Conv2d(16, 64, 5, bias=False)\n",
    "        self.conv2_bn = nn.BatchNorm2d(64)\n",
    "        self.lif2 = snn.Leaky(self.beta, threshold=self.thr, spike_grad=self.spike_grad)\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 10, bias=False)\n",
    "        self.lif3 = snn.Leaky(self.beta, threshold=self.thr, spike_grad=self.spike_grad)\n",
    "        self.dropout = nn.Dropout(self.p1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden states and outputs at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk3_rec = []\n",
    "        mem3_rec = []\n",
    "\n",
    "        # Forward pass\n",
    "        for step in range(self.num_steps):\n",
    "            cur1 = F.avg_pool2d(self.conv1(x), 2)\n",
    "            if self.batch_norm:\n",
    "                cur1 = self.conv1_bn(cur1)\n",
    "\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = F.avg_pool2d(self.conv2(spk1), 2)\n",
    "            if self.batch_norm:\n",
    "                cur2 = self.conv2_bn(cur2)\n",
    "\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            cur3 = self.dropout(self.fc1(spk2.flatten(1)))\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            spk3_rec.append(spk3)\n",
    "            mem3_rec.append(mem3)\n",
    "\n",
    "        return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)\n",
    "\n",
    "net = Net(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise-aware training\n",
    "# code adapted from https://github.com/EIS-Hub/Memristor-Aware-Training\n",
    "\n",
    "class Noisy_Inference(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Function taking the weight tensor as input and applying gaussian noise with standard deviation\n",
    "    (noise_sd) and outputing the noisy version for the forward pass, but keeping track of the\n",
    "    original de-noised version of the weight for the backward pass\n",
    "    \"\"\"\n",
    "    noise_sd = 0e-1 # Change the strength of the noise to be injected into the forwared weights here\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we add some noise from a gaussian distribution\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward( input )\n",
    "        weight = input.clone()\n",
    "        delta_w = 2*torch.abs( weight ).max()\n",
    "        # sd of the sum of two gaussians, given we have pos and neg devices in the chips\n",
    "        # delta_w = torch.sqrt( delta_w**2 + delta_w**2 )\n",
    "        noise = torch.randn_like( weight )*( Noisy_Inference.noise_sd * delta_w )\n",
    "        return torch.add( weight, noise )\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we simply copy the gradient from upward in the computational graph\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        weight = input.clone()\n",
    "        return grad_output\n",
    "# noiser = Noisy_Inference.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T15:45:59.780596Z",
     "iopub.status.busy": "2025-05-02T15:45:59.780294Z",
     "iopub.status.idle": "2025-05-02T15:45:59.788207Z",
     "shell.execute_reply": "2025-05-02T15:45:59.787530Z",
     "shell.execute_reply.started": "2025-05-02T15:45:59.780574Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(config, net, trainloader, criterion, optimizer, device=device, scheduler=None):\n",
    "    \"\"\"Complete one epoch of training.\"\"\"\n",
    "\n",
    "    net.train()\n",
    "    loss_accum = []\n",
    "    i = 0\n",
    "    for data, labels in trainloader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        spk_rec, _ = net(data)\n",
    "        loss = criterion(spk_rec, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        loss_accum.append(loss.item() / config[\"num_steps\"])\n",
    "\n",
    "    return loss_accum\n",
    "\n",
    "def test(config, net, testloader, device=device):\n",
    "    \"\"\"Calculate accuracy on full test set.\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs, _ = net(images)\n",
    "            accuracy = SF.accuracy_rate(outputs, labels)\n",
    "            total += labels.size(0)\n",
    "            correct += accuracy * labels.size(0)\n",
    "\n",
    "    return 100 * correct / total, correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T15:45:59.789341Z",
     "iopub.status.busy": "2025-05-02T15:45:59.789050Z",
     "iopub.status.idle": "2025-05-02T15:45:59.803778Z",
     "shell.execute_reply": "2025-05-02T15:45:59.802968Z",
     "shell.execute_reply.started": "2025-05-02T15:45:59.789313Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(),\n",
    "    lr=config[\"lr\"], betas=config[\"betas\"]\n",
    ")\n",
    "criterion = SF.mse_count_loss(correct_rate=config[\"correct_rate\"],\n",
    "    incorrect_rate=config[\"incorrect_rate\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T15:45:59.804913Z",
     "iopub.status.busy": "2025-05-02T15:45:59.804668Z",
     "iopub.status.idle": "2025-05-03T00:20:17.999540Z",
     "shell.execute_reply": "2025-05-03T00:20:17.998059Z",
     "shell.execute_reply.started": "2025-05-02T15:45:59.804895Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "# Early stopping parameters\n",
    "patience = 5 # Number of epochs to wait before stopping\n",
    "min_delta = 0.00001 # Minimum change in loss to qualify as an improvement\n",
    "patience_counter = 0\n",
    "best_loss = float('inf')\n",
    "\n",
    "print(f\"=======Training Network=======\")\n",
    "# Train\n",
    "for epoch in range(config['num_epochs']):\n",
    "    loss = train(config, net, trainloader, criterion, optimizer,\n",
    "                 device\n",
    "                )\n",
    "    loss_list = loss_list + loss\n",
    "    # Use the average loss of the epoch for early stopping\n",
    "    avg_loss = sum(loss) / len(loss) # Calculate the average loss\n",
    "    # avg_loss = loss[-1]  # Alternatively, use the last loss value\n",
    "    # Test\n",
    "    test_accuracy, correct, total = test(config, net, testloader, device)\n",
    "    print(f\"Epoch: {epoch} \\tTest Accuracy: {test_accuracy} \\tLoss: {avg_loss}\")\n",
    "\n",
    "    # Early stopping\n",
    "    # if avg_loss < best_loss - min_delta:\n",
    "    #     best_loss = avg_loss\n",
    "    #     patience_counter = 0\n",
    "    # else:\n",
    "    #     patience_counter += 1\n",
    "    #     if patience_counter >= patience:\n",
    "    #         print(f\"Early stopping at epoch {epoch}\")\n",
    "    #         torch.save(net.state_dict(), f'fmnist_fp32.pt')\n",
    "    #         break\n",
    "torch.save(net.state_dict(), f'fmnist_fp32_.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
