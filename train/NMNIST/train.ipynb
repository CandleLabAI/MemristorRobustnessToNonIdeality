{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install libraries first\n",
    "!pip install snntorch\n",
    "!pip install tonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import urllib.request\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional as SF\n",
    "\n",
    "import tonic\n",
    "import tonic.transforms as transforms\n",
    "\n",
    "from torch import randn_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset transforms\n",
    "sensor_size = tonic.datasets.NMNIST.sensor_size\n",
    "transforms = transforms.Compose([\n",
    "    transforms.Denoise(filter_time=10000),\n",
    "    transforms.ToFrame(sensor_size=sensor_size, n_time_bins=100),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/nmnist' # Directory where NMNIST dataset is stored\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") # Use GPU if available\n",
    "\n",
    "nmnist_train = tonic.datasets.NMNIST(data_path, train=True, transform=transforms)\n",
    "nmnist_test = tonic.datasets.NMNIST(data_path, train=False, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"num_epochs\": 100,  # Number of epochs to train for (per trial)\n",
    "    \"batch_size\": 128,  # Batch size\n",
    "    \"seed\": 0,  # Random seed\n",
    "\n",
    "    # Network parameters\n",
    "    \"batch_norm\": True,  # Whether or not to use batch normalization\n",
    "    \"dropout\": 0.09,  # Dropout rate\n",
    "    \"beta\": 0.92,  # Decay rate parameter (beta)\n",
    "    \"threshold\": 2.0,  # Threshold parameter (theta)\n",
    "    \"lr\": 1.9e-3,  # Initial learning rate\n",
    "    \"slope\": 6.0,  # Slope value (k)\n",
    "\n",
    "    # Fixed params\n",
    "    \"num_steps\": 100,  # Number of timesteps to encode input for\n",
    "    \"correct_rate\": 0.8,  # Correct rate\n",
    "    \"incorrect_rate\": 0.2,  # Incorrect rate\n",
    "    \"betas\": (0.9, 0.999),  # Adam optimizer beta valuese\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = config[\"batch_size\"]\n",
    "\n",
    "cached_train = tonic.DiskCachedDataset(nmnist_train, cache_path='/temp/dvsgesture/train')\n",
    "cached_test = tonic.DiskCachedDataset(nmnist_test, cache_path='/temp/dvsgesture/test')\n",
    "\n",
    "trainloader = DataLoader(cached_train, shuffle=True, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False))\n",
    "testloader = DataLoader(cached_test, shuffle=True, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False))\n",
    "frames, target = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.thr = config[\"threshold\"]\n",
    "        self.slope = config[\"slope\"]\n",
    "        self.beta = config[\"beta\"]\n",
    "        self.num_steps = config[\"num_steps\"]\n",
    "        self.batch_norm = config[\"batch_norm\"]\n",
    "        self.p1 = config[\"dropout\"]\n",
    "        self.spike_grad = surrogate.fast_sigmoid(self.slope)\n",
    "        # self.init_net()\n",
    "\n",
    "        # Initialize Layers\n",
    "        self.conv1 = nn.Conv2d(2, 16, 5, bias=False)\n",
    "        self.conv1_bn = nn.BatchNorm2d(16)\n",
    "        self.lif1 = snn.Leaky(self.beta, threshold=self.thr, spike_grad=self.spike_grad)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5, bias=False)\n",
    "        self.conv2_bn = nn.BatchNorm2d(32)\n",
    "        self.lif2 = snn.Leaky(self.beta, threshold=self.thr, spike_grad=self.spike_grad)\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 10, bias=False)\n",
    "        self.lif3 = snn.Leaky(self.beta, threshold=self.thr, spike_grad=self.spike_grad)\n",
    "        self.dropout = nn.Dropout(self.p1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden states and outputs at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk3_rec = []\n",
    "        mem3_rec = []\n",
    "\n",
    "        # Forward pass\n",
    "\n",
    "        for step in range(self.num_steps):\n",
    "            cur1 = F.avg_pool2d(self.conv1(x[step]), 2)\n",
    "            if self.batch_norm:\n",
    "                cur1 = self.conv1_bn(cur1)\n",
    "\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = F.avg_pool2d(self.conv2(spk1), 2)\n",
    "            if self.batch_norm:\n",
    "                cur2 = self.conv2_bn(cur2)\n",
    "\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            cur3 = self.dropout(self.fc1(spk2.flatten(1)))\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            spk3_rec.append(spk3)\n",
    "            mem3_rec.append(mem3)\n",
    "\n",
    "        return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)\n",
    "\n",
    "net = Net(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noisy_Inference(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Function taking the weight tensor as input and applying gaussian noise with standard deviation\n",
    "    (noise_sd) and outputing the noisy version for the forward pass, but keeping track of the\n",
    "    original de-noised version of the weight for the backward pass\n",
    "    \"\"\"\n",
    "    noise_sd = 1.0e-1 # Change the strength of the noise to be injected into the forwared weights here\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we add some noise from a gaussian distribution\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward( input )\n",
    "        weight = input.clone()\n",
    "        delta_w = 2*torch.abs( weight ).max()\n",
    "        # sd of the sum of two gaussians, given we have pos and neg devices in the chips\n",
    "        # delta_w = torch.sqrt( delta_w**2 + delta_w**2 )\n",
    "        noise = torch.randn_like(weight)*( Noisy_Inference.noise_sd * delta_w )\n",
    "        return torch.add( weight, noise )\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we simply copy the gradient from upward in the computational graph\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        weight = input.clone()\n",
    "        return grad_output\n",
    "noiser = Noisy_Inference.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(),\n",
    "    lr=config[\"lr\"], betas=config[\"betas\"]\n",
    ")\n",
    "\n",
    "criterion = SF.mse_count_loss(correct_rate=config[\"correct_rate\"],\n",
    "    incorrect_rate=config[\"incorrect_rate\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, net, trainloader, criterion, optimizer, device=\"cuda\", scheduler=None):\n",
    "    \"\"\"Complete one epoch of training.\"\"\"\n",
    "\n",
    "    net.train()\n",
    "    loss_accum = []\n",
    "    i = 0\n",
    "    for data, labels in trainloader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        # print(data.shape)\n",
    "        spk_rec, _ = net(data.permute(0, 1, 2, 3, 4))\n",
    "        loss = criterion(spk_rec, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        loss_accum.append(loss.item() / config[\"num_steps\"])\n",
    "\n",
    "    return loss_accum\n",
    "\n",
    "def test(config, net, testloader, device=\"cuda\"):\n",
    "    \"\"\"Calculate accuracy on full test set.\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs, _ = net(images.permute(0, 1, 2, 3, 4))\n",
    "            accuracy = SF.accuracy_rate(outputs, labels)\n",
    "            total += labels.size(0)\n",
    "            correct += accuracy * labels.size(0)\n",
    "\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-18T05:35:18.236Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "\n",
    "\n",
    "patience = 5 # Number of epochs to wait before stopping\n",
    "min_delta = 0.0005 # Minimum change in loss to qualify as an improvement\n",
    "patience_counter = 0\n",
    "best_loss = float('inf')\n",
    "print(f\"=======Training Network=======\")\n",
    "# Train\n",
    "for epoch in range(config['num_epochs']):\n",
    "    loss = train(config, net, trainloader, criterion, optimizer,\n",
    "                 device\n",
    "                )\n",
    "    loss_list[i] = loss_list[i] + loss\n",
    "    \n",
    "    # Use the average loss of the epoch for early stopping\n",
    "    avg_loss = sum(loss) / len(loss) # Calculate the average loss\n",
    "    # avg_loss = loss[-1]  # Alternatively, use the last loss value\n",
    "    # Test\n",
    "    test_accuracy = test(config, net, testloader, device)\n",
    "    print(f\"Epoch: {epoch} \\tTest Accuracy: {test_accuracy} \\tLoss: {avg_loss}\")\n",
    "    if avg_loss < best_loss - min_delta:\n",
    "        best_loss = avg_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            torch.save(net.state_dict(), f'model.pt')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
